{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
    "\n",
    "- \n",
    "\n",
    "- taking into account keyboard layout and associated misspellings;\n",
    "- efficiency improvement to make the solution faster;\n",
    "- ...\n",
    "\n",
    "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
    "\n",
    "##### IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:37:00.416577Z",
     "iopub.status.busy": "2025-02-25T20:37:00.416259Z",
     "iopub.status.idle": "2025-02-25T20:37:00.421530Z",
     "shell.execute_reply": "2025-02-25T20:37:00.420240Z",
     "shell.execute_reply.started": "2025-02-25T20:37:00.416551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import time\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:37:03.046484Z",
     "iopub.status.busy": "2025-02-25T20:37:03.046147Z",
     "iopub.status.idle": "2025-02-25T20:38:35.653177Z",
     "shell.execute_reply": "2025-02-25T20:38:35.652035Z",
     "shell.execute_reply.started": "2025-02-25T20:37:03.046459Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300000it [00:50, 5957.30it/s]\n",
      "300000it [00:42, 7104.21it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(\"/kaggle/input/nlp-assignment-1-training-dataset/eng_news_2024_300K-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) == 2:\n",
    "            _, sentence = parts\n",
    "\n",
    "            # Remove citations and references like [1], (2024), etc.\n",
    "            sentence = re.sub(r\"\\[\\d+\\]\", \"\", sentence)\n",
    "            sentence = re.sub(r\"\\(\\d{4}\\)\", \"\", sentence)\n",
    "        \n",
    "            # Remove special characters, numbers, and punctuation\n",
    "            sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)\n",
    "        \n",
    "            # Lowercase and remove extra spaces\n",
    "            sentence = sentence.lower().strip()\n",
    "            sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "\n",
    "            tokens = word_tokenize(sentence)\n",
    "            sentences.append(tokens)\n",
    "\n",
    "with open(\"/kaggle/input/nlp-assignment-1-training-dataset/eng-simple_wikipedia_2021_300K-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) == 2:\n",
    "            _, sentence = parts\n",
    "\n",
    "            # Remove citations and references like [1], (2024), etc.\n",
    "            sentence = re.sub(r\"\\[\\d+\\]\", \"\", sentence)\n",
    "            sentence = re.sub(r\"\\(\\d{4}\\)\", \"\", sentence)\n",
    "        \n",
    "            # Remove special characters, numbers, and punctuation\n",
    "            sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)\n",
    "        \n",
    "            # Lowercase and remove extra spaces\n",
    "            sentence = sentence.lower().strip()\n",
    "            sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "\n",
    "            tokens = word_tokenize(sentence)\n",
    "            sentences.append(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:38:35.654621Z",
     "iopub.status.busy": "2025-02-25T20:38:35.654292Z",
     "iopub.status.idle": "2025-02-25T20:38:36.154092Z",
     "shell.execute_reply": "2025-02-25T20:38:36.152978Z",
     "shell.execute_reply.started": "2025-02-25T20:38:35.654579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "large_dataset = True\n",
    "if large_dataset:\n",
    "    text = \". \".join([\" \".join(tokens) for tokens in sentences])\n",
    "else:\n",
    "    training_file = \"/kaggle/input/bigtxt/big.txt\"\n",
    "    with open(training_file, encoding='utf-8') as f:\n",
    "        text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:38:36.156512Z",
     "iopub.status.busy": "2025-02-25T20:38:36.156142Z",
     "iopub.status.idle": "2025-02-25T20:38:36.176360Z",
     "shell.execute_reply": "2025-02-25T20:38:36.174860Z",
     "shell.execute_reply.started": "2025-02-25T20:38:36.156474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LangModel:\n",
    "    def __init__(self, k=0.01):\n",
    "        # Smoothing parameter.\n",
    "        self.k = k\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = []\n",
    "        self.unigrams = Counter()\n",
    "        self.bigrams = Counter()\n",
    "        self.trigrams = Counter()\n",
    "        self.total_words = 0\n",
    "        self.vocab_size = 0\n",
    "        self.unknown_word_id = -1\n",
    "        \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenizes the input text into sentences and words.\"\"\"\n",
    "        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "        return [re.findall(r'\\b\\w+\\b', sentence) for sentence in sentences]\n",
    "\n",
    "    def get_word_id(self, word):\n",
    "        \"\"\"Returns the numeric ID for a given word, creating a new ID if unseen.\"\"\"\n",
    "        if word not in self.word_to_id:\n",
    "            word_id = len(self.word_to_id)\n",
    "            self.word_to_id[word] = word_id\n",
    "            self.id_to_word.append(word)\n",
    "        return self.word_to_id[word]\n",
    "\n",
    "    def train(self, text):\n",
    "        \"\"\"Train the language model from a given text file.\"\"\"\n",
    "        sentences = self.tokenize_text(text.lower())\n",
    "        for sentence in sentences:\n",
    "            word_ids = [self.get_word_id(word) for word in sentence]\n",
    "            self.total_words += len(word_ids)\n",
    "            \n",
    "            for i, wid in enumerate(word_ids):\n",
    "                self.unigrams[wid] += 1\n",
    "                if i < len(word_ids) - 1:\n",
    "                    self.bigrams[(word_ids[i], word_ids[i+1])] += 1\n",
    "                if i < len(word_ids) - 2:\n",
    "                    self.trigrams[(word_ids[i], word_ids[i+1], word_ids[i+2])] += 1\n",
    "        \n",
    "        self.vocab_size = len(self.word_to_id)\n",
    "        print(f\"Total sentences: {len(sentences)}\")\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Total words: {self.total_words}\")\n",
    "        return True\n",
    "\n",
    "    def get_smoothed_probability(self, numerator, denominator):\n",
    "        \"\"\"Applies Laplace smoothing and returns the probability.\"\"\"\n",
    "        return (numerator + self.k) / (denominator + self.k * self.vocab_size)\n",
    "    \n",
    "    def get_gram1_prob(self, word_id):\n",
    "        \"\"\"Returns the unigram probability with smoothing. If word_id is unknown, we assume it has a count of 0.\"\"\"\n",
    "        count = self.unigrams.get(word_id, 0)\n",
    "        return self.get_smoothed_probability(count, self.total_words)\n",
    "    \n",
    "    def get_gram2_prob(self, word1_id, word2_id):\n",
    "        \"\"\"Returns the bigram probability with smoothing.\"\"\"\n",
    "        if word1_id == self.unknown_word_id or word2_id == self.unknown_word_id:\n",
    "            return self.k / (self.total_words + self.k)\n",
    "    \n",
    "        unigram_count = self.unigrams.get(word1_id, 0)\n",
    "        bigram_count = self.bigrams.get((word1_id, word2_id), 0)\n",
    "    \n",
    "        # Ensure bigram count does not exceed unigram count\n",
    "        bigram_count = min(bigram_count, unigram_count)\n",
    "    \n",
    "        return self.get_smoothed_probability(bigram_count, unigram_count + self.total_words)\n",
    "    \n",
    "    def get_gram3_prob(self, word1_id, word2_id, word3_id):\n",
    "        \"\"\"Returns the trigram probability with smoothing.\"\"\"\n",
    "        if any(word_id == self.unknown_word_id for word_id in (word1_id, word2_id, word3_id)):\n",
    "            return self.k / (self.total_words + self.k)\n",
    "    \n",
    "        bigram_count = self.bigrams.get((word1_id, word2_id), 0)\n",
    "        trigram_count = self.trigrams.get((word1_id, word2_id, word3_id), 0)\n",
    "    \n",
    "        # Ensure trigram count does not exceed bigram count\n",
    "        trigram_count = min(trigram_count, bigram_count)\n",
    "    \n",
    "        return self.get_smoothed_probability(trigram_count, bigram_count + self.total_words)\n",
    "\n",
    "    def score(self, words):\n",
    "        \"\"\"Computes the log probability score for a sequence of words.\"\"\"\n",
    "        # Convert words to IDs (use unknown_word_id if the word is not known)\n",
    "        sentence = [self.word_to_id.get(w, self.unknown_word_id) for w in words]\n",
    "        # Append two unknown word IDs.\n",
    "        sentence.extend([self.unknown_word_id] * 2)\n",
    "        \n",
    "        if not sentence:\n",
    "            return float('-inf')\n",
    "        \n",
    "        result = 0.0\n",
    "        # For each trigram window, add log probabilities.\n",
    "        for i in range(len(sentence) - 2):\n",
    "            prob1 = self.get_gram1_prob(sentence[i])\n",
    "            prob2 = self.get_gram2_prob(sentence[i], sentence[i + 1])\n",
    "            prob3 = self.get_gram3_prob(sentence[i], sentence[i + 1], sentence[i + 2])\n",
    "            # Avoid taking log(0) by using a very small number.\n",
    "            result += math.log(prob1 if prob1 > 0 else 1e-10)\n",
    "            result += math.log(prob2 if prob2 > 0 else 1e-10)\n",
    "            result += math.log(prob3 if prob3 > 0 else 1e-10)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:38:36.178473Z",
     "iopub.status.busy": "2025-02-25T20:38:36.178124Z",
     "iopub.status.idle": "2025-02-25T20:38:36.202905Z",
     "shell.execute_reply": "2025-02-25T20:38:36.201768Z",
     "shell.execute_reply.started": "2025-02-25T20:38:36.178444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_deletes1(word):\n",
    "    \"\"\"Return all strings formed by deleting one character from 'word'.\"\"\"\n",
    "    return [word[:i] + word[i+1:] for i in range(len(word)) if (word[:i] + word[i+1:])]\n",
    "\n",
    "def get_deletes2(word):\n",
    "    \"\"\"Return a list of lists; each inner list contains candidates from a two‐level deletion.\"\"\"\n",
    "    results = []\n",
    "    for i in range(len(word)):\n",
    "        nw = word[:i] + word[i+1:]\n",
    "        if nw:\n",
    "            group = get_deletes1(nw)\n",
    "            group.append(nw)\n",
    "            results.append(group)\n",
    "    return results\n",
    "\n",
    "\n",
    "class SpellCorrector:\n",
    "    def __init__(self, lang_model):\n",
    "        self.lang_model = lang_model  \n",
    "\n",
    "        # Penalty parameters for candidate scoring\n",
    "        self.known_words_penalty = 20.0\n",
    "        self.unknown_words_penalty = 5.0\n",
    "        self.max_candidates_to_check = 14\n",
    "\n",
    "\n",
    "    def word_is_known(self, word):\n",
    "        \"\"\"Return True if the word is known by the language model.\"\"\"\n",
    "        return word in self.lang_model.word_to_id\n",
    "\n",
    "    def edits(self, word):\n",
    "        \"\"\"Generate candidate corrections using two-level deletions.\"\"\"\n",
    "        result = set()\n",
    "        dels = get_deletes2(word)\n",
    "        for group in dels:\n",
    "            for cand in group:\n",
    "                if self.word_is_known(cand):\n",
    "                    result.add(cand)\n",
    "        return list(result)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"\"\"Generate candidate corrections using one-level edits: deletion, transposition, replacement, and insertion.\"\"\"\n",
    "        alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        \n",
    "        deletes = {L + R[1:] for L, R in splits if R}\n",
    "        transposes = {L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1}\n",
    "        replaces = {L + c + R[1:] for L, R in splits if R for c in alphabet}\n",
    "        inserts = {L + c + R for L, R in splits for c in alphabet}\n",
    "        \n",
    "        return list(deletes | transposes | replaces | inserts)\n",
    "\n",
    "    def filter_candidates_by_frequency(self, unique_candidates, orig_word):\n",
    "        \"\"\"If too many candidate corrections exist, only keep the ones with the highest frequency.\"\"\"\n",
    "        if len(unique_candidates) <= self.max_candidates_to_check:\n",
    "            return\n",
    "        \n",
    "        candidate_counts = [(self.lang_model.unigrams.get(self.lang_model.word_to_id.get(cand), 0), cand)\n",
    "                            for cand in unique_candidates]\n",
    "        \n",
    "        candidate_counts.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        unique_candidates.clear()\n",
    "        for i in range(min(self.max_candidates_to_check, len(candidate_counts))):\n",
    "            unique_candidates.add(candidate_counts[i][1])\n",
    "        unique_candidates.add(orig_word)\n",
    "\n",
    "    def get_candidates(self, sentence, position):\n",
    "        \"\"\"Return a list of candidate words for the target word in the sentence.\"\"\"\n",
    "        if position >= len(sentence):\n",
    "            return []\n",
    "\n",
    "        orig_word = sentence[position]\n",
    "        \n",
    "        # Try first the edits2 candidates.\n",
    "        candidates = self.edits2(orig_word)\n",
    "        first_level = True\n",
    "        if not candidates:\n",
    "            candidates = self.edits(orig_word)\n",
    "            first_level = False\n",
    "\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        # If the original word is known, add it.\n",
    "        known_word = self.word_is_known(orig_word)\n",
    "        candidates.append(orig_word)\n",
    "\n",
    "        # Remove duplicates and filter by frequency.\n",
    "        unique_candidates = set(candidates)\n",
    "        self.filter_candidates_by_frequency(unique_candidates, orig_word)\n",
    "\n",
    "        scored_candidates = []\n",
    "        for cand in unique_candidates:\n",
    "            context = self._build_context(sentence, position, cand)\n",
    "            score = self.lang_model.score(context)\n",
    "            score = self._apply_scoring_penalties(score, orig_word, known_word, first_level, cand)\n",
    "            scored_candidates.append((cand, score))\n",
    "\n",
    "        # Sort candidates by descending score.\n",
    "        scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [cand for cand, _ in scored_candidates]\n",
    "\n",
    "    def _build_context(self, sentence, position, candidate):\n",
    "        \"\"\"Build a small context window around the candidate word.\"\"\"\n",
    "        context = []\n",
    "        for i, w in enumerate(sentence):\n",
    "            if i == position:\n",
    "                context.append(candidate)\n",
    "            elif (i < position and i + 2 >= position) or (i > position and i <= position + 2):\n",
    "                context.append(w)\n",
    "        return context\n",
    "\n",
    "    def _apply_scoring_penalties(self, score, orig_word, known_word, first_level, candidate):\n",
    "        \"\"\"Apply penalties based on the known/unknown word status and correction level.\"\"\"\n",
    "        if candidate != orig_word:\n",
    "            if known_word:\n",
    "                if first_level:\n",
    "                    score -= self.known_words_penalty\n",
    "                else:\n",
    "                    score *= 50.0\n",
    "            else:\n",
    "                score -= self.unknown_words_penalty\n",
    "        return score\n",
    "\n",
    "    def fix_fragment(self, text):\n",
    "        \"\"\"Correct a text fragment while preserving some original formatting.\"\"\"\n",
    "        # Split into tokens (words + punctuation)\n",
    "        tokens = re.findall(r\"\\w+[’']*\\w*|[.!?,;:]\", text)\n",
    "        \n",
    "        # Correct words while leaving punctuation untouched\n",
    "        corrected_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.isalpha():\n",
    "                # Process only alphabetic tokens\n",
    "                corrected = self._correct_word(token, tokens, len(corrected_tokens))\n",
    "                corrected_tokens.append(corrected)\n",
    "            else:\n",
    "                # Keep punctuation as-is\n",
    "                corrected_tokens.append(token)\n",
    "        \n",
    "        # Reconstruct the text with original spacing (simplified)\n",
    "        return ' '.join(corrected_tokens).replace(' ,', ',').replace(' .', '.')\n",
    "\n",
    "    def _correct_word(self, word, words, index):\n",
    "        \"\"\"Correct a single word in the sentence.\"\"\"\n",
    "        candidates = self.get_candidates(words, index)\n",
    "        if candidates:\n",
    "            corrected = candidates[0]\n",
    "            if word.istitle():\n",
    "                corrected = corrected.capitalize()\n",
    "            return corrected\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:38:36.204385Z",
     "iopub.status.busy": "2025-02-25T20:38:36.204039Z",
     "iopub.status.idle": "2025-02-25T20:39:13.679121Z",
     "shell.execute_reply": "2025-02-25T20:39:13.678028Z",
     "shell.execute_reply.started": "2025-02-25T20:38:36.204359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 600000\n",
      "Vocabulary size: 236442\n",
      "Total words: 9924458\n",
      "Model trained successfully!\n",
      "Spell corrector created!\n"
     ]
    }
   ],
   "source": [
    "model = LangModel()\n",
    "if model.train(text):\n",
    "    print(\"Model trained successfully!\")\n",
    "\n",
    "corrector = SpellCorrector(model)\n",
    "print(\"Spell corrector created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we can see here, our model is a little bit sensitive for punctuation (spaces btw them), the same for Norvig. Therefore, I decided to use dataset without punctuation for validation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:39:13.680592Z",
     "iopub.status.busy": "2025-02-25T20:39:13.680253Z",
     "iopub.status.idle": "2025-02-25T20:39:13.716414Z",
     "shell.execute_reply": "2025-02-25T20:39:13.715205Z",
     "shell.execute_reply.started": "2025-02-25T20:39:13.680553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True He likes to eat apple.\n",
      "False The doesn't know how to fix the problem.\n",
      "iPHone is expensive.\n",
      "Or. Smth went to paris.\n",
      "Wait... did you mean that ? ?\n",
      "state of the art technology.\n",
      "Hello, it’s nice here ! she said.\n"
     ]
    }
   ],
   "source": [
    "print(corrector.fix_fragment(\"He likes to eat applle.\") == \"He likes to eat apple.\", corrector.fix_fragment(\"He likes to eat applle.\"))\n",
    "print(corrector.fix_fragment(\"She doesn't know how to fix teh problem.\") == \"She doesn't know how to fix the problem.\", corrector.fix_fragment(\"She doesn't know how to fix teh problem.\"))\n",
    "print(corrector.fix_fragment(\"iPHone is expensive.\"))\n",
    "print(corrector.fix_fragment(\"Mr. Smth went to paris.\"))\n",
    "print(corrector.fix_fragment(\"Wait... did you mean that??\"))\n",
    "print(corrector.fix_fragment(\"state-of-the-arte technology.\"))\n",
    "print(corrector.fix_fragment(\"“Hello, it’s nice here!” she said.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "# Dataset\n",
    "I decided to collect [300k Wikipedia + 300k News](https://wortschatz.uni-leipzig.de/en/download/English) sentences of the last year, and preprocess it removing citations and references (like [1], (2024), etc.), special characters, numbers, and punctuation. And after that lowercasing and removing extra spaces\n",
    "\n",
    "For evaluating I take  [2014 European Union WEB 10k](https://wortschatz.uni-leipzig.de/en/download/English) dataset. Preprocessing was the same. After that I go through the sentences and generate typos with some probability\n",
    "# Choosing n in ngram model\n",
    "The choice of n is cruicial in ngram model, since it strictly leads to accuracy. While searching in articles, I found that mostly n = 2-3 showed best performance. I took n = 3 to have more context for training.\n",
    "# Smoothing\n",
    "To avoid recieving zero probability for n-gram that was not found in the training text, I applied Laplace smoothing. The model generalizes better with it when faced with new or rare word combinations.\n",
    "# Generate correction candidates\n",
    "1. Primary I generate correction using Insertions, Replacements, Transpositions, and Deletions. This broad search ensures most plausible corrections are considered.\n",
    "2. Since the depth = 2 recursive algorithm of 1st metod is slow and generates many candidates, I decided to save only 2 depth deletion. So, if first method didn't show valid candidate I use two-level deletions (e.g., \"test\" → \"tst\" → \"st\"). \n",
    "3. If upper method didn't worked give original word\n",
    "# Candidate selection\n",
    "Due to large amount of correction candidates generation, we need somehow to bound them. So I filter candidates based on their frequency of occurrence and take only first 14. This frequency-based filtering ensures that only the most probable candidates are considered, improving efficiency and reducing the computational load.\n",
    "# Penalties\n",
    "To avoid over-correcting valid known words I added penalties to prefer existing, more probable words. Additional penalty is applied to make corrections allow for likely typos but still slightly penalize them to prefer the original word if it’s valid (e.g., rare words, names). Moreover, there exist multiplication pinalty. It reflects that two deletions are less likely to be valid corrections unless absolutely necessary.\n",
    "# Context\n",
    "Each candidate word's score is computed by considering its probability in the given context, which is determined by the trigram model. The final score combines the likelihood of the candidate in the context with penalties for known and unknown words.\n",
    "# Efficiency\n",
    "A key challenge in this task is the need to handle large datasets and quickly generate candidate corrections. To improve efficiency, I limited the number of candidates considered and used a simple heuristic to filter candidates by frequency. This reduces the number of computations and speeds up the spelling correction process.\n",
    "\n",
    "Additionally, the model uses a pre-built word_to_id dictionary to quickly look up word IDs, which further speeds up both training and inference phases.\n",
    "# Keyboard Layout and Misspellings\n",
    "I didn't implement it in my approach\n",
    "\n",
    "# My approach\n",
    "My approach was based on N-grams method with slightly modifications: reducing number of generation correction candidates, smoothing, filtering and bounding candidates. Moreover adding penalties to make context better and reduce misspelling correct words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:39:13.717951Z",
     "iopub.status.busy": "2025-02-25T20:39:13.717551Z",
     "iopub.status.idle": "2025-02-25T20:39:15.441790Z",
     "shell.execute_reply": "2025-02-25T20:39:15.440930Z",
     "shell.execute_reply.started": "2025-02-25T20:39:13.717888Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:01, 5889.64it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluate_sentences = []\n",
    "\n",
    "with open(\"/kaggle/input/nlp-assignment-1-training-dataset/eng-eu_web_2014_10K-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) == 2:\n",
    "            _, sentence = parts\n",
    "\n",
    "            # Remove citations and references like [1], (2024), etc.\n",
    "            sentence = re.sub(r\"\\[\\d+\\]\", \"\", sentence)\n",
    "            sentence = re.sub(r\"\\(\\d{4}\\)\", \"\", sentence)\n",
    "        \n",
    "            # Remove special characters, numbers, and punctuation\n",
    "            sentence = re.sub(r\"[^a-zA-Z\\s]\", \"\", sentence)\n",
    "        \n",
    "            # Lowercase and remove extra spaces\n",
    "            sentence = sentence.lower().strip()\n",
    "            sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "\n",
    "            tokens = word_tokenize(sentence)\n",
    "            evaluate_sentences.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:39:15.443975Z",
     "iopub.status.busy": "2025-02-25T20:39:15.443668Z",
     "iopub.status.idle": "2025-02-25T20:39:15.455241Z",
     "shell.execute_reply": "2025-02-25T20:39:15.454241Z",
     "shell.execute_reply.started": "2025-02-25T20:39:15.443940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "number_sentences = 1000\n",
    "evaluate_sentences = evaluate_sentences[:number_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:39:15.456724Z",
     "iopub.status.busy": "2025-02-25T20:39:15.456353Z",
     "iopub.status.idle": "2025-02-25T20:39:16.062010Z",
     "shell.execute_reply": "2025-02-25T20:39:16.060965Z",
     "shell.execute_reply.started": "2025-02-25T20:39:15.456698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('/kaggle/input/bigtxt/big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def correct_sentence_function(sentence):\n",
    "    \"\"\"Corrects a sentence by applying word-level correction.\"\"\"\n",
    "    words = sentence.split()\n",
    "    return ' '.join([correction(word) for word in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T20:39:16.063351Z",
     "iopub.status.busy": "2025-02-25T20:39:16.063000Z",
     "iopub.status.idle": "2025-02-25T20:42:11.866391Z",
     "shell.execute_reply": "2025-02-25T20:42:11.865339Z",
     "shell.execute_reply.started": "2025-02-25T20:39:16.063317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total percent of errored words: 13.58706273206957\n",
      "N-gram model: fix-rate = 76.77094570298453 ; broken-rate = 1.2556185264803597 ; words-per-second = 1583.5481727525726\n",
      "Norvig model: fix-rate = 67.92520676015822 ; broken-rate = 5.804182137971468 ; words-per-second = 125.71400391755107\n"
     ]
    }
   ],
   "source": [
    "TYPO_PROB = 0.03  # Chance of making a typo for a single letter\n",
    "SECOND_TYPO_CF = 0.2  # Chance of making two typos, relative to TYPO_PROB\n",
    "REPLACE_PROB = 0.7\n",
    "INSERT_PROB = 0.1\n",
    "REMOVE_PROB = 0.1\n",
    "TRANSPOSE_PROB = 0.1\n",
    "\n",
    "def add_typos(word):\n",
    "    if not word:\n",
    "        return word\n",
    "\n",
    "    typo_word = list(word)\n",
    "    typo_applied = False\n",
    "\n",
    "    i = 0\n",
    "    while i < len(typo_word):\n",
    "        if random.random() < TYPO_PROB:\n",
    "            typo_applied = True\n",
    "            typo_type = random.choices(\n",
    "                ['replace', 'insert', 'remove', 'transpose'],\n",
    "                [REPLACE_PROB, INSERT_PROB, REMOVE_PROB, TRANSPOSE_PROB]\n",
    "            )[0]\n",
    "\n",
    "            if typo_type == 'replace' and len(typo_word) > 1:\n",
    "                typo_word[i] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            elif typo_type == 'insert':\n",
    "                typo_word.insert(i, random.choice('abcdefghijklmnopqrstuvwxyz'))\n",
    "                i += 1\n",
    "            elif typo_type == 'remove' and len(typo_word) > 1:\n",
    "                typo_word.pop(i)\n",
    "                i -= 1\n",
    "            elif typo_type == 'transpose' and i + 1 < len(typo_word):\n",
    "                typo_word[i], typo_word[i + 1] = typo_word[i + 1], typo_word[i]\n",
    "                i += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(typo_word) if typo_applied else word\n",
    "\n",
    "def generate_typo_sentences(sentences):\n",
    "    typo_sentences = []\n",
    "    typo_flags = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        typo_sentence = []\n",
    "        typo_flag = []\n",
    "        for word in sentence:\n",
    "            typo_word = add_typos(word)\n",
    "            typo_sentence.append(typo_word)\n",
    "            typo_flag.append(typo_word != word)\n",
    "        typo_sentences.append(typo_sentence)\n",
    "        typo_flags.append(typo_flag)\n",
    "\n",
    "    return typo_sentences, typo_flags\n",
    "\n",
    "def assess_spell_checker(original, typo, processed, flags):\n",
    "    errored, fixed, broken = 0, 0, 0\n",
    "    total_words = sum(len(s) for s in original)\n",
    "\n",
    "    for orig, typo, proc, flag in zip(original, typo, processed, flags):\n",
    "        for o, t, p, f in zip(orig, typo, proc, flag):\n",
    "            if f:\n",
    "                errored += 1\n",
    "                if p == o:\n",
    "                    fixed += 1\n",
    "            elif p != o:\n",
    "                broken += 1\n",
    "\n",
    "    error_rate = (errored / total_words) * 100\n",
    "    fix_rate = (fixed / errored * 100) if errored else 0\n",
    "    broken_rate = (broken / total_words) * 100\n",
    "\n",
    "    return {\n",
    "        \"errors\": error_rate,\n",
    "        \"fix_rate\": fix_rate,\n",
    "        \"broken\": broken_rate\n",
    "    }\n",
    "\n",
    "def evaluate_spell_checkers(sentences):\n",
    "    typo_sentences, typo_flags = generate_typo_sentences(sentences)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    processed_1 = [corrector.fix_fragment(\" \".join(s)).split() for s in typo_sentences]\n",
    "    assessment_1 = assess_spell_checker(sentences, typo_sentences, processed_1, typo_flags)\n",
    "    processing_time_1 = time.time() - start_time\n",
    "  \n",
    "    start_time = time.time()\n",
    "    processed_2 = [correct_sentence_function(\" \".join(s)).split() for s in typo_sentences]\n",
    "    assessment_2 = assess_spell_checker(sentences, typo_sentences, processed_2, typo_flags)\n",
    "    processing_time_2 = time.time() - start_time\n",
    "\n",
    "    total_words = sum(len(s) for s in sentences)\n",
    "    words_per_second_1 = total_words / processing_time_1 if processing_time_1 > 0 else 0\n",
    "    words_per_second_2 = total_words / processing_time_2 if processing_time_2 > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"model_1\": assessment_1,\n",
    "        \"model_2\": assessment_2,\n",
    "        \"words_per_second_1\": words_per_second_1,\n",
    "        \"words_per_second_2\": words_per_second_2\n",
    "    }\n",
    "\n",
    "result = evaluate_spell_checkers(evaluate_sentences)\n",
    "print(\"Total percent of errored words:\", result['model_1']['errors'])\n",
    "print(\"N-gram model: fix-rate =\", result['model_1']['fix_rate'], \"; broken-rate =\", result['model_1']['broken'], \"; words-per-second =\", result[\"words_per_second_1\"])\n",
    "print(\"Norvig model: fix-rate =\", result['model_2']['fix_rate'], \"; broken-rate =\", result['model_2']['broken'], \"; words-per-second =\", result[\"words_per_second_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we can see my model is 10x faster than classic norvig model, fix rate better in average 9-10%, and broken-rate doesn't exeed 2% compared to 5-7% from Norvig.\n",
    "\n",
    "# Overall my solution is much more better than simple Norvig approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful resources (also included in the archive in moodle):\n",
    "\n",
    "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
    "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6724597,
     "datasetId": 3833686,
     "sourceId": 6640776,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11207744,
     "datasetId": 6734642,
     "sourceId": 10844958,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11217883,
     "datasetId": 6734707,
     "sourceId": 10854157,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11212935,
     "datasetId": 6733979,
     "sourceId": 10849724,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
